{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-12 10:41:55--  https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/071/024/original/SteelPlant.csv\n",
      "Resolving d2beiqkhq929f0.cloudfront.net (d2beiqkhq929f0.cloudfront.net)... 108.158.41.214, 108.158.41.222, 108.158.41.226, ...\n",
      "Connecting to d2beiqkhq929f0.cloudfront.net (d2beiqkhq929f0.cloudfront.net)|108.158.41.214|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3133750 (3.0M) [text/plain]\n",
      "Saving to: ‘SteelPlant.csv’\n",
      "\n",
      "SteelPlant.csv      100%[===================>]   2.99M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2024-11-12 10:41:55 (41.1 MB/s) - ‘SteelPlant.csv’ saved [3133750/3133750]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "!wget https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/071/024/original/SteelPlant.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('SteelPlant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r1    c2\n",
       "r2    c2\n",
       "r3    c2\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        [0, 1, -2],\n",
    "        [2, 3, -4],\n",
    "        [-4, 5, -6]\n",
    "    ],\n",
    "    columns = ['c1', 'c2', 'c3'],\n",
    "    index=['r1', 'r2', 'r3']\n",
    ").idxmax(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target'] = df.iloc[:, -7:].idxmax(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function entropy in module scipy.stats._entropy:\n",
      "\n",
      "entropy(pk: 'np.typing.ArrayLike', qk: 'np.typing.ArrayLike | None' = None, base: 'float | None' = None, axis: 'int' = 0, *, nan_policy='propagate', keepdims=False) -> 'np.number | np.ndarray'\n",
      "    Calculate the Shannon entropy/relative entropy of given distribution(s).\n",
      "\n",
      "    If only probabilities `pk` are given, the Shannon entropy is calculated as\n",
      "    ``H = -sum(pk * log(pk))``.\n",
      "\n",
      "    If `qk` is not None, then compute the relative entropy\n",
      "    ``D = sum(pk * log(pk / qk))``. This quantity is also known\n",
      "    as the Kullback-Leibler divergence.\n",
      "\n",
      "    This routine will normalize `pk` and `qk` if they don't sum to 1.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    pk : array_like\n",
      "        Defines the (discrete) distribution. Along each axis-slice of ``pk``,\n",
      "        element ``i`` is the  (possibly unnormalized) probability of event\n",
      "        ``i``.\n",
      "    qk : array_like, optional\n",
      "        Sequence against which the relative entropy is computed. Should be in\n",
      "        the same format as `pk`.\n",
      "    base : float, optional\n",
      "        The logarithmic base to use, defaults to ``e`` (natural logarithm).\n",
      "    axis : int or None, default: 0\n",
      "        If an int, the axis of the input along which to compute the statistic.\n",
      "        The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
      "        corresponding element of the output.\n",
      "        If ``None``, the input will be raveled before computing the statistic.\n",
      "    nan_policy : {'propagate', 'omit', 'raise'}\n",
      "        Defines how to handle input NaNs.\n",
      "\n",
      "        - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
      "          which the  statistic is computed, the corresponding entry of the output\n",
      "          will be NaN.\n",
      "        - ``omit``: NaNs will be omitted when performing the calculation.\n",
      "          If insufficient data remains in the axis slice along which the\n",
      "          statistic is computed, the corresponding entry of the output will be\n",
      "          NaN.\n",
      "        - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
      "    keepdims : bool, default: False\n",
      "        If this is set to True, the axes which are reduced are left\n",
      "        in the result as dimensions with size one. With this option,\n",
      "        the result will broadcast correctly against the input array.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    S : {float, array_like}\n",
      "        The calculated entropy.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    Informally, the Shannon entropy quantifies the expected uncertainty\n",
      "    inherent in the possible outcomes of a discrete random variable.\n",
      "    For example,\n",
      "    if messages consisting of sequences of symbols from a set are to be\n",
      "    encoded and transmitted over a noiseless channel, then the Shannon entropy\n",
      "    ``H(pk)`` gives a tight lower bound for the average number of units of\n",
      "    information needed per symbol if the symbols occur with frequencies\n",
      "    governed by the discrete distribution `pk` [1]_. The choice of base\n",
      "    determines the choice of units; e.g., ``e`` for nats, ``2`` for bits, etc.\n",
      "\n",
      "    The relative entropy, ``D(pk|qk)``, quantifies the increase in the average\n",
      "    number of units of information needed per symbol if the encoding is\n",
      "    optimized for the probability distribution `qk` instead of the true\n",
      "    distribution `pk`. Informally, the relative entropy quantifies the expected\n",
      "    excess in surprise experienced if one believes the true distribution is\n",
      "    `qk` when it is actually `pk`.\n",
      "\n",
      "    A related quantity, the cross entropy ``CE(pk, qk)``, satisfies the\n",
      "    equation ``CE(pk, qk) = H(pk) + D(pk|qk)`` and can also be calculated with\n",
      "    the formula ``CE = -sum(pk * log(qk))``. It gives the average\n",
      "    number of units of information needed per symbol if an encoding is\n",
      "    optimized for the probability distribution `qk` when the true distribution\n",
      "    is `pk`. It is not computed directly by `entropy`, but it can be computed\n",
      "    using two calls to the function (see Examples).\n",
      "\n",
      "    See [2]_ for more information.\n",
      "\n",
      "    Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
      "    code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
      "    this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
      "    rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
      "    arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
      "    masked array with ``mask=False``.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Shannon, C.E. (1948), A Mathematical Theory of Communication.\n",
      "           Bell System Technical Journal, 27: 379-423.\n",
      "           https://doi.org/10.1002/j.1538-7305.1948.tb01338.x\n",
      "    .. [2] Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information\n",
      "           Theory (Wiley Series in Telecommunications and Signal Processing).\n",
      "           Wiley-Interscience, USA.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    The outcome of a fair coin is the most uncertain:\n",
      "\n",
      "    >>> import numpy as np\n",
      "    >>> from scipy.stats import entropy\n",
      "    >>> base = 2  # work in units of bits\n",
      "    >>> pk = np.array([1/2, 1/2])  # fair coin\n",
      "    >>> H = entropy(pk, base=base)\n",
      "    >>> H\n",
      "    1.0\n",
      "    >>> H == -np.sum(pk * np.log(pk)) / np.log(base)\n",
      "    True\n",
      "\n",
      "    The outcome of a biased coin is less uncertain:\n",
      "\n",
      "    >>> qk = np.array([9/10, 1/10])  # biased coin\n",
      "    >>> entropy(qk, base=base)\n",
      "    0.46899559358928117\n",
      "\n",
      "    The relative entropy between the fair coin and biased coin is calculated\n",
      "    as:\n",
      "\n",
      "    >>> D = entropy(pk, qk, base=base)\n",
      "    >>> D\n",
      "    0.7369655941662062\n",
      "    >>> D == np.sum(pk * np.log(pk/qk)) / np.log(base)\n",
      "    True\n",
      "\n",
      "    The cross entropy can be calculated as the sum of the entropy and\n",
      "    relative entropy`:\n",
      "\n",
      "    >>> CE = entropy(pk, base=base) + entropy(pk, qk, base=base)\n",
      "    >>> CE\n",
      "    1.736965594166206\n",
      "    >>> CE == -np.sum(pk * np.log(qk)) / np.log(base)\n",
      "    True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "help(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.364097425566414)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = df['Target'].value_counts(normalize=True)\n",
    "entropy(p, base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       "Other_Faults    0.340288\n",
       "Bumps           0.247724\n",
       "K_Scatch        0.178521\n",
       "Pastry          0.118841\n",
       "Z_Scratch       0.059837\n",
       "Stains          0.029554\n",
       "Dirtiness       0.025235\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
